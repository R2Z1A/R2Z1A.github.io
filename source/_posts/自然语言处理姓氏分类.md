---
title: 自然语言处理姓氏分类
date: 2024-06-09 22:17:51
categories: [其他垃圾, 作业]
tags: [人工智能, 自然语言处理]
excerpt: 基于pytorch搭建模型，根据姓氏进行国籍分类。
thumbnail: /images/blog/nlp_surname/titleimg.jpg
mathjax: true
---

{% notel blue 'fa-solid fa-book' '前言' %}
面向学校作业的博客撰写...发布在这个**人迹罕至**的小网页肯定没人看吧。以后当投机经验点子回收站了。
{% endnotel %}

- [概要](#概要)
- [内容](#内容)
  - [数据集](#数据集)
    - [简介](#简介)
    - [预处理](#预处理)
  - [多层感知机姓氏分类](#多层感知机姓氏分类)
    - [多层感知机](#多层感知机)
    - [多层感知机模型定义](#多层感知机模型定义)
    - [多层感知机模型训练](#多层感知机模型训练)
    - [多层感知机模型测试](#多层感知机模型测试)
  - [CNN姓氏分类](#cnn姓氏分类)
    - [CNN卷积网络](#cnn卷积网络)
    - [CNN模型定义](#cnn模型定义)
    - [CNN模型训练](#cnn模型训练)
    - [CNN模型测试](#cnn模型测试)
- [总结](#总结)

## 概要

本文主要有：

1. 介绍使用的数据集，进行预处理。
2. 结合自然语言处理，简要学习pytorch里的多层感知机和cnn。
3. 构建多层感知机模型和cnn模型，应用于数据集的分类任务。

## 内容

### 数据集

#### 简介

本文使用的数据集为The Surname Dataset姓氏数据集，收集了来自18个不同国家的10,000个姓氏，这些姓氏是作者从互联网上不同的姓名来源收集的。

通过代码快速预览数据集：

```python
import collections
import numpy as np
import pandas as pd
import re

from argparse import Namespace

args = Namespace(
    raw_dataset_csv="data/surnames/surnames.csv",
    train_proportion=0.7,
    val_proportion=0.15,
    test_proportion=0.15,
    output_munged_csv="data/surnames/surnames_with_splits.csv",
    seed=1337
)
# Read raw data
surnames = pd.read_csv(args.raw_dataset_csv, header=0)

surnames.head()
```

打印输出为：

|    | surname |   nationality |
| :----- | --: | ------: |
| 0 |  Woodford  | English |
| 1 |  Coté  | French |
| 2 |  Kore  | English |
| 3 |  Koury  | Arabic |
| 4 |  Lebzak  | Russian |

可以看到数据集组成比较简单，其中surname为features，nationality为labels。为多分类问题任务。

#### 预处理

接下来，我们划分数据集为训练集、验证集、测试集三部分。

```python
# Splitting train by nationality
# Create dict
by_nationality = collections.defaultdict(list)
for _, row in surnames.iterrows():
    by_nationality[row.nationality].append(row.to_dict())

# Create split data
final_list = []
np.random.seed(args.seed)
for _, item_list in sorted(by_nationality.items()):
    np.random.shuffle(item_list)
    n = len(item_list)
    n_train = int(args.train_proportion*n)
    n_val = int(args.val_proportion*n)
    n_test = int(args.test_proportion*n)
    
    # Give data point a split attribute
    for item in item_list[:n_train]:
        item['split'] = 'train'
    for item in item_list[n_train:n_train+n_val]:
        item['split'] = 'val'
    for item in item_list[n_train+n_val:]:
        item['split'] = 'test'  
    
    # Add to final list
    final_list.extend(item_list)

# Write split data to file
final_surnames = pd.DataFrame(final_list)

final_surnames.split.value_counts()
```

打印输出为：

>
>train    7680
>
>test     1660
>
>val      1640
>
>Name: split, dtype: int64

这里，我们将split作为标签插入数据集中进行分类。最后，保存分割后的数据集：

```python
# Write munged data to CSV
final_surnames.to_csv(args.output_munged_csv, index=False)
```

### 多层感知机姓氏分类

#### 多层感知机

多层感知机由输入层、隐藏层、输出层组成，结构简单。通过层级之间连接的线性运算和神经元激活函数的非线性运算，这种顺序神经网络拥有强大的拟合能力（映射）。

![多层感知机](/images/blog/nlp_surname/mlp.png)

>多层感知机模型示意图

以异或问题为例，单层感知机无法处理异或问题，我们可以观察多层感知机是如何处理异或问题的：

![异或示意](https://yifdu.github.io/2018/12/20/Natural-Language-Processing-with-PyTorch%EF%BC%88%E5%9B%9B%EF%BC%89/MLP_2.png)
>MLP对异或问题的处理过程，对模型各层做可视化处理。从左到右为：输入；第一层的连接前向传播运算；第一层经激活函数处理后的输出；第二层的连接前向传播运算。

可以看到，MLP自发的学习到异或分类问题的特征，自发的将原始输入进行多次映射变换，从而将线性不可分问题（左）转化为线性可分问题（右）。

类似的，将MLP应用于此姓氏分类问题，我们也可以期望神经网络模型可以通过多次映射将姓氏特征空间转化为“线性可分”，从而根据姓氏进行国籍分类。

#### 多层感知机模型定义

在这里，我们用pytorch定义了一个多层感知机模型，用其可以很好的处理异或问题。当然，我们的重点并不是处理异或问题。

```python
class MultilayerPerceptron(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        """
        Args:
            input_dim (int): the size of the input vectors
            hidden_dim (int): the output size of the first Linear layer
            output_dim (int): the output size of the second Linear layer
        """
        """
        初始化多层感知器模型，包括两个线性层。
        参数：
            input_dim (int): 输入向量的大小
            hidden_dim (int): 第一个线性层的输出大小
            output_dim (int): 第二个线性层的输出大小
        """
        super(MultilayerPerceptron, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x_in, apply_softmax=False):
        """The forward pass of the MLP
    
        Args:
            x_in (torch.Tensor): an input data tensor.
                x_in.shape should be (batch, input_dim)
            apply_softmax (bool): a flag for the softmax activation
                should be false if used with the Cross Entropy losses
        Returns:
            the resulting tensor. tensor.shape should be (batch, output_dim)
        """
        """多层感知器的前向传播过程

        参数：
            x_in (torch.Tensor): 输入数据张量。
                x_in的形状应为 (batch, input_dim)
            apply_softmax (bool): softmax激活函数的标志
                如果与交叉熵损失一起使用，应为false
        返回：
            结果张量。张量的形状应为 (batch, output_dim)
        """
        intermediate = F.relu(self.fc1(x_in))
        output = self.fc2(intermediate)

        # 如果激活softmax
        if apply_softmax:
            output = F.softmax(output, dim=1)
        return output
```

下面是针对姓氏分类的多层感知机模型，可以看到其和异或问题的MLP基本类似，不同的是其输入数据形式不同。

```python
class SurnameVectorizer(object):
    """ The Vectorizer which coordinates the Vocabularies and puts them to use"""
    """ 姓氏向量化器，负责协调词汇表并将它们应用到实际中 """
    def __init__(self, surname_vocab, nationality_vocab):
        self.surname_vocab = surname_vocab
        self.nationality_vocab = nationality_vocab

    def vectorize(self, surname):
        """Vectorize the provided surname

        Args:
            surname (str): the surname
        Returns:
            one_hot (np.ndarray): a collapsed one-hot encoding
        """
        """将提供的姓氏进行向量化

        参数：
            surname (str): 姓氏字符串
        返回：
            one_hot (np.ndarray): 一个折叠的独热编码
        """
        vocab = self.surname_vocab # 获取姓氏词汇表
        one_hot = np.zeros(len(vocab), dtype=np.float32)
        for token in surname:
            one_hot[vocab.lookup_token(token)] = 1 # 将对应字符在词汇表中的索引位置设为1
        return one_hot

    @classmethod
    def from_dataframe(cls, surname_df):
        """Instantiate the vectorizer from the dataset dataframe

        Args:
            surname_df (pandas.DataFrame): the surnames dataset
        Returns:
            an instance of the SurnameVectorizer
        """
        """从数据集的数据框实例化向量化器

        参数：
            surname_df (pandas.DataFrame): 姓氏数据集
        返回：
            一个SurnameVectorizer实例
        """
        # 创建姓氏词汇表，未知字符用"@"表示
        surname_vocab = Vocabulary(unk_token="@")
        # 创建国籍词汇表，不添加未知字符
        nationality_vocab = Vocabulary(add_unk=False)

        for index, row in surname_df.iterrows():
            for letter in row.surname:
                surname_vocab.add_token(letter) # 将字符添加到姓氏词汇表中
            nationality_vocab.add_token(row.nationality) # 将国籍添加到国籍词汇表中

        return cls(surname_vocab, nationality_vocab)
```

#### 多层感知机模型训练

我们不能将字符串直接作为输入给模型训练。这里我们使用词向量：

```python
class SurnameDataset(Dataset):
    # Implementation is nearly identical to Section 3.5

    def __getitem__(self, index):
        row = self._target_df.iloc[index]
        # 将姓氏转换为向量表示
        surname_vector = \
            self._vectorizer.vectorize(row.surname)
         # 查找国籍对应的索引
        nationality_index = \
            self._vectorizer.nationality_vocab.lookup_token(row.nationality)

        return {'x_surname': surname_vector,
                'y_nationality': nationality_index}

class SurnameVectorizer(object):
    """ The Vectorizer which coordinates the Vocabularies and puts them to use"""
    """ 姓氏向量化器，负责协调词汇表并将它们应用到实际中 """
    def __init__(self, surname_vocab, nationality_vocab):
        self.surname_vocab = surname_vocab
        self.nationality_vocab = nationality_vocab

    def vectorize(self, surname):
        """Vectorize the provided surname

        Args:
            surname (str): the surname
        Returns:
            one_hot (np.ndarray): a collapsed one-hot encoding
        """
        """将提供的姓氏进行向量化

        参数：
            surname (str): 姓氏字符串
        返回：
            one_hot (np.ndarray): 一个折叠的独热编码
        """
        vocab = self.surname_vocab # 获取姓氏词汇表
        one_hot = np.zeros(len(vocab), dtype=np.float32)
        for token in surname:
            one_hot[vocab.lookup_token(token)] = 1 # 将对应字符在词汇表中的索引位置设为1
        return one_hot

    @classmethod
    def from_dataframe(cls, surname_df):
        """Instantiate the vectorizer from the dataset dataframe

        Args:
            surname_df (pandas.DataFrame): the surnames dataset
        Returns:
            an instance of the SurnameVectorizer
        """
        """从数据集的数据框实例化向量化器

        参数：
            surname_df (pandas.DataFrame): 姓氏数据集
        返回：
            一个SurnameVectorizer实例
        """
        # 创建姓氏词汇表，未知字符用"@"表示
        surname_vocab = Vocabulary(unk_token="@")
        # 创建国籍词汇表，不添加未知字符
        nationality_vocab = Vocabulary(add_unk=False)

        for index, row in surname_df.iterrows():
            for letter in row.surname:
                surname_vocab.add_token(letter) # 将字符添加到姓氏词汇表中
            nationality_vocab.add_token(row.nationality) # 将国籍添加到国籍词汇表中

        return cls(surname_vocab, nationality_vocab)
```

接下来是模型训练的参数以及训练过程的代码：

```python
args = Namespace(
    # 数据和路径信息
    surname_csv="data/surnames/surnames_with_splits.csv",  # 姓氏CSV文件路径
    vectorizer_file="vectorizer.json",  # 向量化器文件路径
    model_state_file="model.pth",  # 模型状态文件路径
    save_dir="model_storage/ch4/surname_mlp",  # 保存目录路径

    # 模型超参数
    hidden_dim=300,  # 隐藏层维度

    # 训练超参数
    seed=1337,  # 随机种子
    num_epochs=100,  # 训练轮数
    early_stopping_criteria=5,  # 早停标准
    learning_rate=0.001,  # 学习率
    batch_size=64,  # 批量大小

    # 运行时选项省略以节省空间
)
# 加载数据集并创建向量化器
dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)
# 获取向量化器
vectorizer = dataset.get_vectorizer()

# 创建姓氏分类器，输入维度为姓氏词汇表长度，隐藏层维度为args.hidden_dim，输出维度为国家/民族词汇表长度
classifier = SurnameClassifier(input_dim=len(vectorizer.surname_vocab),
                               hidden_dim=args.hidden_dim,
                               output_dim=len(vectorizer.nationality_vocab))

# 将分类器移动到指定设备（CPU或GPU）上
classifier = classifier.to(args.device)    

# 创建交叉熵损失函数，使用数据集中的类别权重
loss_func = nn.CrossEntropyLoss(dataset.class_weights)
# 创建优化器，使用Adam算法，学习率为args.learning_rate
optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,
                                                 mode='min', factor=0.5,
                                                 patience=1)

train_state = make_train_state(args)

# 初始化训练进度条
epoch_bar = tqdm_notebook(desc='training routine', 
                          total=args.num_epochs,
                          position=0)

# 设置数据集为训练集
dataset.set_split('train')
# 初始化训练批次进度条
train_bar = tqdm_notebook(desc='split=train',
                          total=dataset.get_num_batches(args.batch_size), 
                          position=1, 
                          leave=True)
# 设置数据集为验证集
dataset.set_split('val')
# 初始化验证批次进度条
val_bar = tqdm_notebook(desc='split=val',
                        total=dataset.get_num_batches(args.batch_size), 
                        position=1, 
                        leave=True)

try:
    # 遍历每个epoch
    for epoch_index in range(args.num_epochs):
        train_state['epoch_index'] = epoch_index

        # 迭代训练数据集

        # 设置：生成批次，将损失和准确率设置为0，设置训练模式
        dataset.set_split('train')
        batch_generator = generate_batches(dataset, 
                                           batch_size=args.batch_size, 
                                           device=args.device)
        running_loss = 0.0
        running_acc = 0.0
        classifier.train()

        # 迭代批次
        for batch_index, batch_dict in enumerate(batch_generator):
            # 训练过程包括以下5个步骤：

            # --------------------------------------
            # 步骤1. 梯度清零
            optimizer.zero_grad()

            # 步骤2. 计算输出
            y_pred = classifier(batch_dict['x_surname'])

            # 步骤3. 计算损失
            loss = loss_func(y_pred, batch_dict['y_nationality'])
            loss_t = loss.item()
            running_loss += (loss_t - running_loss) / (batch_index + 1)

            # 步骤4. 使用损失计算梯度
            loss.backward()

            # 步骤5. 使用优化器进行梯度更新
            optimizer.step()
            # -----------------------------------------
            # 计算准确率
            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])
            running_acc += (acc_t - running_acc) / (batch_index + 1)

            # 更新进度条
            train_bar.set_postfix(loss=running_loss, acc=running_acc, 
                            epoch=epoch_index)
            train_bar.update()

        train_state['train_loss'].append(running_loss)
        train_state['train_acc'].append(running_acc)

        # 迭代验证数据集

        # 设置：生成批次，将损失和准确率设置为0；设置评估模式
        dataset.set_split('val')
        batch_generator = generate_batches(dataset, 
                                           batch_size=args.batch_size, 
                                           device=args.device)
        running_loss = 0.
        running_acc = 0.
        classifier.eval()

        # 迭代批次
        for batch_index, batch_dict in enumerate(batch_generator):

            # 计算输出
            y_pred =  classifier(batch_dict['x_surname'])

            # 步骤3. 计算损失
            loss = loss_func(y_pred, batch_dict['y_nationality'])
            loss_t = loss.to("cpu").item()
            running_loss += (loss_t - running_loss) / (batch_index + 1)

            # 计算准确率
            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])
            running_acc += (acc_t - running_acc) / (batch_index + 1)
            val_bar.set_postfix(loss=running_loss, acc=running_acc, 
                            epoch=epoch_index)
            val_bar.update()

        train_state['val_loss'].append(running_loss)
        train_state['val_acc'].append(running_acc)

        # 更新训练状态
        train_state = update_train_state(args=args, model=classifier,
                                         train_state=train_state)

        # 调整学习率
        scheduler.step(train_state['val_loss'][-1])

        # 如果满足早停条件，则跳出循环
        if train_state['stop_early']:
            break

        # 重置进度条计数器
        train_bar.n = 0
        val_bar.n = 0
        epoch_bar.update()
except KeyboardInterrupt:
    print("Exiting loop")

```

#### 多层感知机模型测试

我们调用`model.eval()`方法进行测试，这可以防止pytorch在测试评估的时候更新模型参数。

```python
classifier.load_state_dict(torch.load(train_state['model_filename']))

classifier = classifier.to(args.device)
dataset.class_weights = dataset.class_weights.to(args.device)
loss_func = nn.CrossEntropyLoss(dataset.class_weights)

dataset.set_split('test')
batch_generator = generate_batches(dataset, 
                                   batch_size=args.batch_size, 
                                   device=args.device)
running_loss = 0.
running_acc = 0.
classifier.eval()

for batch_index, batch_dict in enumerate(batch_generator):
    # compute the output
    y_pred =  classifier(batch_dict['x_surname'])
    
    # compute the loss
    loss = loss_func(y_pred, batch_dict['y_nationality'])
    loss_t = loss.item()
    running_loss += (loss_t - running_loss) / (batch_index + 1)

    # compute the accuracy
    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])
    running_acc += (acc_t - running_acc) / (batch_index + 1)

train_state['test_loss'] = running_loss
train_state['test_acc'] = running_acc

print("Test loss: {};".format(train_state['test_loss']))
print("Test Accuracy: {}".format(train_state['test_acc']))
```

> Test loss: 1.7944978427886964;
>
> Test Accuracy: 47.0

接下来，我们提供两种函数`predict_nationality()`和`predict_topk_nationalit()`，分别就最大概率和前K个最大概率输出预测结果。

```python
# 定义一个函数predict_nationality，输入参数为name（名字），classifier（分类器）和vectorizer（向量化工具）
def predict_nationality(name, classifier, vectorizer):
    # 使用vectorizer将名字向量化
    vectorized_name = vectorizer.vectorize(name)
    # 将向量化的名字转换为torch张量，并调整形状
    vectorized_name = torch.tensor(vectorized_name).view(1, -1)
    # 使用分类器对向量化的名字进行预测，并应用softmax激活函数
    result = classifier(vectorized_name, apply_softmax=True)

    # 获取概率值和索引
    probability_values, indices = result.max(dim=1)
    # 获取索引的数值
    index = indices.item()

    # 使用vectorizer的nationality_vocab查找索引对应的国籍
    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)
    # 获取概率值的数值
    probability_value = probability_values.item()

    # 返回预测结果，包括国籍和概率值
    return {'nationality': predicted_nationality,
            'probability': probability_value}

# 定义一个函数，用于预测名字对应的国籍概率最高的前k个结果
def predict_topk_nationality(name, classifier, vectorizer, k=5):
    # 将输入的名字向量化
    vectorized_name = vectorizer.vectorize(name)
    # 将向量化后的名字转换为PyTorch张量，并调整形状为(1, -1)
    vectorized_name = torch.tensor(vectorized_name).view(1, -1)
    # 使用分类器对向量化后的名字进行预测，得到预测向量
    prediction_vector = classifier(vectorized_name, apply_softmax=True)
    # 对预测向量进行top-k操作，得到概率值和对应的索引
    probability_values, indices = torch.topk(prediction_vector, k=k)

    # 将返回的概率值和索引从PyTorch张量转换为NumPy数组，并去掉多余的维度
    probability_values = probability_values.detach().numpy()[0]
    indices = indices.detach().numpy()[0]

    # 初始化结果列表
    results = []
    # 遍历概率值和索引，将它们映射回国籍，并将结果添加到结果列表中
    for prob_value, index in zip(probability_values, indices):
        nationality = vectorizer.nationality_vocab.lookup_index(index)
        results.append({'nationality': nationality,
                        'probability': prob_value})

    # 返回结果列表
    return results

    new_surname = input("Enter a surname to classify: ")
    classifier = classifier.to("cpu")
    prediction = predict_nationality(new_surname, classifier, vectorizer)
    print("{} -> {} (p={:0.2f})".format(new_surname,
                                        prediction['nationality'],
                                        prediction['probability']))

```

>Enter a surname to classify:  Cai
>
>Cai -> Vietnamese (p=0.34)

```python
new_surname = input("Enter a surname to classify: ")
classifier = classifier.to("cpu")

k = int(input("How many of the top predictions to see? "))
if k > len(vectorizer.nationality_vocab):
    print("Sorry! That's more than the # of nationalities we have.. defaulting you to max size :)")
    k = len(vectorizer.nationality_vocab)
    
predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)

print("Top {} predictions:".format(k))
print("===================")
for prediction in predictions:
    print("{} -> {} (p={:0.2f})".format(new_surname,
                                        prediction['nationality'],
                                        prediction['probability']))
```

>Enter a surname to classify:  Cai
>
>How many of the top predictions to see?  5
>
>Top 5 predictions:
>
>===================
>
>Cai -> Vietnamese (p=0.34)
>
>Cai -> Korean (p=0.17)
>
>Cai -> Chinese (p=0.15)
>
>Cai -> Italian (p=0.13)
>
>Cai -> Portuguese (p=0.08)

我们对'蔡徐坤'的蔡(cai)进行分类，可以看到模型效果不好，对chinese的分类可能性为第三，这可能与数据集数据质量有关。

### CNN姓氏分类

#### CNN卷积网络

卷积神经网络（CNN）是一种在图像和视频处理领域广泛应用的深度学习模型，具有强大的自动特征提取能力。它通过卷积层、池化层和全连接层的叠加，能够有效捕捉空间和时间依赖性信息，从而在各种任务中取得卓越性能。卷积神经网络（CNN）的概念最早由Yann LeCun等人提出，并成功应用于手写数字识别任务。其具有参数共享（卷积核）、稀疏连接、移动不变性（特征描述）特点，能很好的提取数据的局部特征。

卷积神经网络主要在计算机视觉领域使用，但是其也可以用于自然语言处理。在2014年，Yoon Kim针对CNN做了一些变形，提出了文本分类模型TextCNN。

![cnnmodel](/images/blog/nlp_surname/cnnmodel.JPG)

可以看到，TextCNN卷积核与传统的用于计算机视觉领域的卷积核不同：TextCNN卷积核尺寸为（n*k，其中n表示感受野，k为词向量大小），而一般卷积核为正方形。

#### CNN模型定义

模型实现时，我们使用1d的卷积核。

```python
class SurnameClassifier(nn.Module):
    def __init__(self, initial_num_channels, num_classes, num_channels):
        """
        Args:
            initial_num_channels (int): size of the incoming feature vector
            num_classes (int): size of the output prediction vector
            num_channels (int): constant channel size to use throughout network
        """
        super(SurnameClassifier, self).__init__()
        
        self.convnet = nn.Sequential(
            nn.Conv1d(in_channels=initial_num_channels, 
                      out_channels=num_channels, kernel_size=3),
            nn.ELU(),
            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, 
                      kernel_size=3, stride=2),
            nn.ELU(),
            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, 
                      kernel_size=3, stride=2),
            nn.ELU(),
            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, 
                      kernel_size=3),
            nn.ELU()
        )
        self.fc = nn.Linear(num_channels, num_classes)

    def forward(self, x_surname, apply_softmax=False):
        """The forward pass of the classifier
        
        Args:
            x_surname (torch.Tensor): an input data tensor. 
                x_surname.shape should be (batch, initial_num_channels, max_surname_length)
            apply_softmax (bool): a flag for the softmax activation
                should be false if used with the Cross Entropy losses
        Returns:
            the resulting tensor. tensor.shape should be (batch, num_classes)
        """
        features = self.convnet(x_surname).squeeze(dim=2)
       
        prediction_vector = self.fc(features)

        if apply_softmax:
            prediction_vector = F.softmax(prediction_vector, dim=1)

        return prediction_vector
```

#### CNN模型训练

训练过程基本同MLP一致。

```python
def make_train_state(args):
    return {'stop_early': False,
            'early_stopping_step': 0,
            'early_stopping_best_val': 1e8,
            'learning_rate': args.learning_rate,
            'epoch_index': 0,
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'test_loss': -1,
            'test_acc': -1,
            'model_filename': args.model_state_file}

def update_train_state(args, model, train_state):
    """Handle the training state updates.

    Components:
     - Early Stopping: Prevent overfitting.
     - Model Checkpoint: Model is saved if the model is better

    :param args: main arguments
    :param model: model to train
    :param train_state: a dictionary representing the training state values
    :returns:
        a new train_state
    """

    # Save one model at least
    if train_state['epoch_index'] == 0:
        torch.save(model.state_dict(), train_state['model_filename'])
        train_state['stop_early'] = False

    # Save model if performance improved
    elif train_state['epoch_index'] >= 1:
        loss_tm1, loss_t = train_state['val_loss'][-2:]

        # If loss worsened
        if loss_t >= train_state['early_stopping_best_val']:
            # Update step
            train_state['early_stopping_step'] += 1
        # Loss decreased
        else:
            # Save the best model
            if loss_t < train_state['early_stopping_best_val']:
                torch.save(model.state_dict(), train_state['model_filename'])

            # Reset early stopping step
            train_state['early_stopping_step'] = 0

        # Stop early ?
        train_state['stop_early'] = \
            train_state['early_stopping_step'] >= args.early_stopping_criteria

    return train_state

args = Namespace(
    # Data and Path information
    surname_csv="data/surnames/surnames_with_splits.csv",
    vectorizer_file="vectorizer.json",
    model_state_file="model.pth",
    save_dir="model_storage/ch4/cnn",
    # Model hyper parameters
    hidden_dim=100,
    num_channels=256,
    # Training hyper parameters
    seed=1337,
    learning_rate=0.001,
    batch_size=128,
    num_epochs=100,
    early_stopping_criteria=5,
    dropout_p=0.1,
    # Runtime options
    cuda=False,
    reload_from_files=False,
    expand_filepaths_to_save_dir=True,
    catch_keyboard_interrupt=True
)


if args.expand_filepaths_to_save_dir:
    args.vectorizer_file = os.path.join(args.save_dir,
                                        args.vectorizer_file)

    args.model_state_file = os.path.join(args.save_dir,
                                         args.model_state_file)
    
    print("Expanded filepaths: ")
    print("\t{}".format(args.vectorizer_file))
    print("\t{}".format(args.model_state_file))
    
# Check CUDA
if not torch.cuda.is_available():
    args.cuda = False

args.device = torch.device("cuda" if args.cuda else "cpu")
print("Using CUDA: {}".format(args.cuda))

def set_seed_everywhere(seed, cuda):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if cuda:
        torch.cuda.manual_seed_all(seed)
        
def handle_dirs(dirpath):
    if not os.path.exists(dirpath):
        os.makedirs(dirpath)
        
# Set seed for reproducibility
set_seed_everywhere(args.seed, args.cuda)

# handle dirs
handle_dirs(args.save_dir)

if args.reload_from_files:
    # training from a checkpoint
    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,
                                                              args.vectorizer_file)
else:
    # create dataset and vectorizer
    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)
    dataset.save_vectorizer(args.vectorizer_file)
    
vectorizer = dataset.get_vectorizer()

classifier = SurnameClassifier(initial_num_channels=len(vectorizer.surname_vocab), 
                               num_classes=len(vectorizer.nationality_vocab),
                               num_channels=args.num_channels)

classifer = classifier.to(args.device)
dataset.class_weights = dataset.class_weights.to(args.device)

loss_func = nn.CrossEntropyLoss(weight=dataset.class_weights)
optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,
                                           mode='min', factor=0.5,
                                           patience=1)

train_state = make_train_state(args)

epoch_bar = tqdm_notebook(desc='training routine', 
                          total=args.num_epochs,
                          position=0)

dataset.set_split('train')
train_bar = tqdm_notebook(desc='split=train',
                          total=dataset.get_num_batches(args.batch_size), 
                          position=1, 
                          leave=True)
dataset.set_split('val')
val_bar = tqdm_notebook(desc='split=val',
                        total=dataset.get_num_batches(args.batch_size), 
                        position=1, 
                        leave=True)

try:
    for epoch_index in range(args.num_epochs):
        train_state['epoch_index'] = epoch_index

        # Iterate over training dataset

        # setup: batch generator, set loss and acc to 0, set train mode on

        dataset.set_split('train')
        batch_generator = generate_batches(dataset, 
                                           batch_size=args.batch_size, 
                                           device=args.device)
        running_loss = 0.0
        running_acc = 0.0
        classifier.train()

        for batch_index, batch_dict in enumerate(batch_generator):
            # the training routine is these 5 steps:

            # --------------------------------------
            # step 1. zero the gradients
            optimizer.zero_grad()

            # step 2. compute the output
            y_pred = classifier(batch_dict['x_surname'])

            # step 3. compute the loss
            loss = loss_func(y_pred, batch_dict['y_nationality'])
            loss_t = loss.item()
            running_loss += (loss_t - running_loss) / (batch_index + 1)

            # step 4. use loss to produce gradients
            loss.backward()

            # step 5. use optimizer to take gradient step
            optimizer.step()
            # -----------------------------------------
            # compute the accuracy
            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])
            running_acc += (acc_t - running_acc) / (batch_index + 1)

            # update bar
            train_bar.set_postfix(loss=running_loss, acc=running_acc, 
                            epoch=epoch_index)
            train_bar.update()

        train_state['train_loss'].append(running_loss)
        train_state['train_acc'].append(running_acc)

        # Iterate over val dataset

        # setup: batch generator, set loss and acc to 0; set eval mode on
        dataset.set_split('val')
        batch_generator = generate_batches(dataset, 
                                           batch_size=args.batch_size, 
                                           device=args.device)
        running_loss = 0.
        running_acc = 0.
        classifier.eval()

        for batch_index, batch_dict in enumerate(batch_generator):

            # compute the output
            y_pred =  classifier(batch_dict['x_surname'])

            # step 3. compute the loss
            loss = loss_func(y_pred, batch_dict['y_nationality'])
            loss_t = loss.item()
            running_loss += (loss_t - running_loss) / (batch_index + 1)

            # compute the accuracy
            acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])
            running_acc += (acc_t - running_acc) / (batch_index + 1)
            val_bar.set_postfix(loss=running_loss, acc=running_acc, 
                            epoch=epoch_index)
            val_bar.update()

        train_state['val_loss'].append(running_loss)
        train_state['val_acc'].append(running_acc)

        train_state = update_train_state(args=args, model=classifier,
                                         train_state=train_state)

        scheduler.step(train_state['val_loss'][-1])

        if train_state['stop_early']:
            break

        train_bar.n = 0
        val_bar.n = 0
        epoch_bar.update()
except KeyboardInterrupt:
    print("Exiting loop")
    
```

#### CNN模型测试

打印模型的损失，有：

>Test loss: 1.9216371824343998;
>
>Test Accuracy: 60.7421875

我们同样预测'蔡徐坤'的cai，模型的前五最大概率输出如下：

>Cai -> Vietnamese (p=0.73)
>
>Cai -> Chinese (p=0.14)
>
>Cai -> Korean (p=0.10)
>
>Cai -> Irish (p=0.01)
>
>Cai -> Italian (p=0.01)

可以看到模型仍然认为蔡徐坤是Vietnamese而不是Chinese，这里认为仍是由于数据集导致分类效果差。

## 总结

在本研究中，我们学习了使用MLP和CNN来解决姓氏分类的问题。通过使用卷积操作来捕捉姓氏中局部字符序列的特征，CNN模型能够显著提升分类的准确率。

通过与MLP模型的对比分析，我们进一步理解了不同模型在处理不同类型数据时的适应性和局限性。MLP更适合处理简单和结构化的数据，而CNN则在处理具有空间结构的数据方面展现出更强的能力。这一发现为我们在实际应用场景中选择最合适的模型提供了重要的参考依据。
