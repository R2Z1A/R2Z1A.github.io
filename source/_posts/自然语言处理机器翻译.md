---
title: 自然语言处理机器翻译
date: 2024-06-25 23:19:38
categories: [其他垃圾, 作业]
tags: [人工智能, 自然语言处理]
excerpt: 基于pytorch，实现简单的机器翻译。
thumbnail: /images/blog/nlp_machinetrans/废物.JPG
mathjax: true
---

{% notel blue 'fa-solid fa-book' '前言' %}
本文介绍了一些小小的理论知识，并在机器翻译章节进行代码实践。
{% endnotel %}

- [概要](#概要)
- [内容](#内容)
  - [编码器-解码器](#编码器-解码器)
  - [搜索](#搜索)
    - [贪婪搜索](#贪婪搜索)
    - [束搜索](#束搜索)
  - [注意力机制](#注意力机制)
    - [计算](#计算)
  - [机器翻译](#机器翻译)
    - [数据处理](#数据处理)
    - [模型实现](#模型实现)
      - [编码器](#编码器)
      - [解码器](#解码器)
    - [训练评估](#训练评估)
- [总结](#总结)

## 概要

1. 介绍编码器-解码器、束搜索、注意力机制、机器翻译。
2. 根据上述内容，进行机器翻译实践。

## 内容

### 编码器-解码器

编码器-解码器用于解决不定长序列问题。编码器用来分析输入序列，解码器用来生成输出序列。

![编码器解码器](/images/blog/nlp_machinetrans/10.9_seq2seq.svg)

编码器-解码器由两部分循环神经网络组成，编码器将各个时间步的隐状态转换为背景变量，而解码器根据背景变量建模当前时间步输出的概率分布。

对模型训练，我们使用最大似然估计，模型loss为：

$$
-\log P(y_1, \ldots, y_{T'} \mid x_1, \ldots, x_T) = -\sum_{t'=1}^{T'} \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \boldsymbol{c}),
$$

在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。我们需要将解码器在上一个时间步的输出作为当前时间步的输入。或者，在训练中我们也可以将标签序列（训练集的真实输出序列）在上一个时间步的标签作为解码器在当前时间步的输入。这叫作强制教学（teacher forcing）。

### 搜索

模型训练好了之后，我们还要使用模型得到预测结果。

在上一节中，我们介绍了编码器解码器模型，假设模型训练完毕，输入一个句子，我们得到的是一堆概率分布。具体的，是所有可能的输出序列及其概率值。

#### 贪婪搜索

如图，假设模型得到了这样一堆概率分布作为输出，贪婪搜索在当前时间步中选择概率最大的词，并进行下一步的搜索。

![prob1](/images/blog/nlp_machinetrans/10.10_s2s_prob1.svg)

然而，贪婪搜索并不能保证全局最优解，事实上，它很容易陷入局部最优。

![prob2](/images/blog/nlp_machinetrans/10.10_s2s_prob2.svg)

#### 束搜索

显然，我们希望搜索到最大概率的序列，但是穷举搜索整个概率分布是不现实的。为了尽可能得到全局最优解，我们使用束搜索。

![束搜索](/images/blog/nlp_machinetrans/10.10_beam_search.svg)

束搜索（beam search）是对贪婪搜索的一个改进算法。它有一个束宽（beam size）超参数。我们将它设为$k$。在时间步1时，选取当前时间步条件概率最大的$k$个词，分别组成$k$个候选输出序列的首词。在之后的每个时间步，基于上个时间步的$k$个候选输出序列，从$k\left|\mathcal{Y}\right|$个可能的输出序列中选取条件概率最大的$k$个，作为该时间步的候选输出序列。最终，我们从各个时间步的候选输出序列中筛选出包含特殊符号“&lt;eos&gt;”的序列，并将它们中所有特殊符号“&lt;eos&gt;”后面的子序列舍弃，得到最终候选输出序列的集合。

可以近似的理解为，束搜索是一种更宽的贪婪搜索————它不仅只看最好的，还看第二好的，第三好的......并在这基础上继续往下搜索。当然，普通的贪婪搜索也可以视为一种束宽为1的束搜索。

### 注意力机制

注意力机制是一种在深度学习中广泛应用的技术，其核心思想是通过关注输入数据中的特定部分来提高模型的性能和效率。

#### 计算

具体来说，令编码器在时间步$t$的隐藏状态为$\boldsymbol{h}_t$，且总时间步数为$T$。那么解码器在时间步$t'$的背景变量为所有编码器隐藏状态的加权平均：

$$
\boldsymbol{c} _ {t'} = \sum_{t=1}^T \alpha_{t' t} \boldsymbol{h}_t,
$$

其中给定$t'$时，权重$\alpha_{t' t}$在$t=1,\ldots,T$的值是一个概率分布。为了得到概率分布，我们可以使用softmax运算:

$$
\alpha_{t' t} = \frac{\exp(e_{t' t})}{ \sum_{k=1}^T \exp(e_{t' k}) },\quad t=1,\ldots,T.
$$

现在，我们需要定义如何计算上式中softmax运算的输入$e_{t' t}$。由于$e_{t' t}$同时取决于解码器的时间步$t'$和编码器的时间步$t$，我们不妨以解码器在时间步$t'-1$的隐藏状态$\boldsymbol{s}_{t' - 1}$与编码器在时间步$t$的隐藏状态$\boldsymbol{h}_t$为输入，并通过函数$a$计算$e_{t' t}$：

$$
e_{t' t} = a(\boldsymbol{s}_{t' - 1}, \boldsymbol{h}_t).
$$

这里函数$a$有多种选择，如果两个输入向量长度相同，一个简单的选择是计算它们的内积$a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。而最早提出注意力机制的论文则将输入连结后通过含单隐藏层的多层感知机变换 [1]：

$$
a(\boldsymbol{s}, \boldsymbol{h}) = \boldsymbol{v}^\top \tanh(\boldsymbol{W}_s \boldsymbol{s} + \boldsymbol{W}_h \boldsymbol{h}),
$$

其中$\boldsymbol{v}$、$\boldsymbol{W}_s$、$\boldsymbol{W}_h$都是可以学习的模型参数。

我们还可以对注意力机制采用更高效的矢量化计算。广义上，注意力机制的输入包括查询项以及一一对应的键项和值项，其中值项是需要加权平均的一组项。在加权平均中，值项的权重来自查询项以及与该值项对应的键项的计算。b

在上面的例子中，查询项为解码器的隐藏状态，键项和值项均为编码器的隐藏状态。让我们考虑一个常见的简单情形，即编码器和解码器的隐藏单元个数均为$h$，且函数$a(\boldsymbol{s}, \boldsymbol{h})=\boldsymbol{s}^\top \boldsymbol{h}$。假设我们希望根据解码器单个隐藏状态$\boldsymbol{s} _ {t' - 1} \in \mathbb{R}^{h}$和编码器所有隐藏状态$\boldsymbol{h} _ t \in \mathbb{R}^{h}, t = 1,\ldots,T$来计算背景向量$\boldsymbol{c} _ {t'}\in \mathbb{R}^{h}$。
我们可以将查询项矩阵$\boldsymbol{Q} \in \mathbb{R}^{1 \times h}$设为$\boldsymbol{s} _ {t' - 1}^\top$，并令键项矩阵$\boldsymbol{K} \in \mathbb{R}^{T \times h}$和值项矩阵$\boldsymbol{V} \in \mathbb{R}^{T \times h}$相同且第$t$行均为$\boldsymbol{h} _ t^\top$。此时，我们只需要通过矢量化计算

$$\text{softmax}(\boldsymbol{Q}\boldsymbol{K}^\top)\boldsymbol{V}$$

即可算出转置后的背景向量$\boldsymbol{c} _ {t'}^\top$。当查询项矩阵$\boldsymbol{Q}$的行数为$n$时，上式将得到$n$行的输出矩阵。输出矩阵与查询项矩阵在相同行上一一对应。

### 机器翻译

#### 数据处理

数据集样式如下：

>elle est vieille . she is old .
>
>elle est tranquille .  she is quiet .
>
>...

我们先定义一些特殊符号。其中“&lt;pad&gt;”（padding）符号用来添加在较短序列后，直到每个序列等长，而“&lt;bos&gt;”和“&lt;eos&gt;”符号分别表示序列的开始和结束。

```python
import collections
import os
import io
import math
import torch
from torch import nn
import torch.nn.functional as F
import torchtext.vocab as Vocab
import torch.utils.data as Data

import sys
# sys.path.append("..") 
import d2lzh_pytorch as d2l

# 定义常量PAD、BOS、EOS，分别表示填充、开始和结束
PAD, BOS, EOS = '<pad>', '<bos>', '<eos>'
# 设置环境变量，指定使用的GPU设备编号
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
# 根据当前设备是否支持CUDA，选择使用CPU或GPU进行计算
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 打印PyTorch版本和使用的计算设备
print(torch.__version__, device)
```

定义两个辅助函数对后面读取的数据进行预处理。

```python
# 将一个序列中所有的词记录在all_tokens中以便之后构造词典，然后在该序列后面添加PAD直到序列
# 长度变为max_seq_len，然后将序列保存在all_seqs中
def process_one_seq(seq_tokens, all_tokens, all_seqs, max_seq_len):
    all_tokens.extend(seq_tokens)
    seq_tokens += [EOS] + [PAD] * (max_seq_len - len(seq_tokens) - 1)
    all_seqs.append(seq_tokens)

# 使用所有的词来构造词典。并将所有序列中的词变换为词索引后构造Tensor
def build_data(all_tokens, all_seqs):
    vocab = Vocab.Vocab(collections.Counter(all_tokens),
                        specials=[PAD, BOS, EOS])
    indices = [[vocab.stoi[w] for w in seq] for seq in all_seqs]
    return vocab, torch.tensor(indices)
```

在这个数据集里，每一行是一对法语句子和它对应的英语句子，中间使用`'\t'`隔开。在读取数据时，我们在句末附上“&lt;eos&gt;”符号，并可能通过添加“&lt;pad&gt;”符号使每个序列的长度均为`max_seq_len`。法语词的索引和英语词的索引相互独立。

```python
def read_data(max_seq_len):
    # 初始化输入和输出的tokens和序列列表
    in_tokens, out_tokens, in_seqs, out_seqs = [], [], [], []
    # 读取文件内容
    with io.open('fr-en-small.txt') as f:
        lines = f.readlines()
    
    # 遍历每一行
    for line in lines:
        # 去除行尾空格，并以制表符分割输入和输出序列
        in_seq, out_seq = line.rstrip().split('\t')
        # 将输入和输出序列按空格分割为tokens
        in_seq_tokens, out_seq_tokens = in_seq.split(' '), out_seq.split(' ')
        # 如果加上EOS后长于max_seq_len，则忽略掉此样本
        if max(len(in_seq_tokens), len(out_seq_tokens)) > max_seq_len - 1:
            continue
        # 处理输入序列
        process_one_seq(in_seq_tokens, in_tokens, in_seqs, max_seq_len)
        # 处理输出序列
        process_one_seq(out_seq_tokens, out_tokens, out_seqs, max_seq_len)
    # 构建输入数据
    in_vocab, in_data = build_data(in_tokens, in_seqs)
    # 构建输出数据
    out_vocab, out_data = build_data(out_tokens, out_seqs)    
    # 返回输入和输出的词汇表以及数据集
    return in_vocab, out_vocab, Data.TensorDataset(in_data, out_data)
```

将序列的最大长度设成7，然后查看读取到的第一个样本。该样本分别包含法语词索引序列和英语词索引序列。

```python
max_seq_len = 7
in_vocab, out_vocab, dataset = read_data(max_seq_len)
dataset[0]
```

>(tensor([ 5,  4, 45,  3,  2,  0,  0]), tensor([ 8,  4, 27,  3,  2,  0,  0]))

#### 模型实现

##### 编码器

在编码器中，我们将输入语言的词索引通过词嵌入层得到词的表征，然后输入到一个多层门控循环单元中。

```python
class Encoder(nn.Module):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 drop_prob=0, **kwargs):
        # 初始化Encoder类，继承自nn.Module
        super(Encoder, self).__init__(**kwargs)
        # 定义嵌入层，将词汇表大小映射到嵌入维度
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # 定义GRU层，输入为嵌入维度，隐藏层大小为num_hiddens，层数为num_layers，dropout概率为drop_prob
        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=drop_prob)


    def forward(self, inputs, state):
        # 输入形状是(批量大小, 时间步数)。将输出互换样本维和时间步维
        embedding = self.embedding(inputs.long()).permute(1, 0, 2) # (seq_len, batch, input_size)
        return self.rnn(embedding, state)

    def begin_state(self):
        return None
```

下面创建编码器，创建一个张量模拟输入，查看输出。

```python
encoder = Encoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)
output, state = encoder(torch.zeros((4, 7)), encoder.begin_state())
output.shape, state.shape # GRU的state是h, 而LSTM的是一个元组(h, c)
```

>(torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))

##### 解码器

下面，实现注意力机制。

```python
def attention_model(input_size, attention_size):
    model = nn.Sequential(nn.Linear(input_size, attention_size, bias=False),
                          nn.Tanh(),
                          nn.Linear(attention_size, 1, bias=False))
    return model

def attention_forward(model, enc_states, dec_state):
    """
    enc_states: (时间步数, 批量大小, 隐藏单元个数)
    dec_state: (批量大小, 隐藏单元个数)
    """
    # 将解码器隐藏状态广播到和编码器隐藏状态形状相同后进行连结
    dec_states = dec_state.unsqueeze(dim=0).expand_as(enc_states)
    enc_and_dec_states = torch.cat((enc_states, dec_states), dim=2)
    e = model(enc_and_dec_states)  # 形状为(时间步数, 批量大小, 1)
    alpha = F.softmax(e, dim=0)  # 在时间步维度做softmax运算
    return (alpha * enc_states).sum(dim=0)  # 返回背景变量
```

在解码器的设计中，我们加入注意力层，通过注意力机制计算当前时间步的背景向量。

```python
class Decoder(nn.Module):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 attention_size, drop_prob=0):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.attention = attention_model(2*num_hiddens, attention_size)
        # GRU的输入包含attention输出的c和实际输入, 所以尺寸是 num_hiddens+embed_size
        self.rnn = nn.GRU(num_hiddens + embed_size, num_hiddens, 
                          num_layers, dropout=drop_prob)
        self.out = nn.Linear(num_hiddens, vocab_size)

    def forward(self, cur_input, state, enc_states):
        """
        cur_input shape: (batch, )
        state shape: (num_layers, batch, num_hiddens)
        """
        # 使用注意力机制计算背景向量
        c = attention_forward(self.attention, enc_states, state[-1])
        # 将嵌入后的输入和背景向量在特征维连结, (批量大小, num_hiddens+embed_size)
        input_and_c = torch.cat((self.embedding(cur_input), c), dim=1) 
        # 为输入和背景向量的连结增加时间步维，时间步个数为1
        output, state = self.rnn(input_and_c.unsqueeze(0), state)
        # 移除时间步维，输出形状为(批量大小, 输出词典大小)
        output = self.out(output).squeeze(dim=0)
        return output, state

    def begin_state(self, enc_state):
        # 直接将编码器最终时间步的隐藏状态作为解码器的初始隐藏状态
        return enc_state
```

#### 训练评估

我们使用强制教学方法，下面是损失函数及训练相关代码。

```python
def batch_loss(encoder, decoder, X, Y, loss):
    batch_size = X.shape[0]
    enc_state = encoder.begin_state()
    enc_outputs, enc_state = encoder(X, enc_state)
    # 初始化解码器的隐藏状态
    dec_state = decoder.begin_state(enc_state)
    # 解码器在最初时间步的输入是BOS
    dec_input = torch.tensor([out_vocab.stoi[BOS]] * batch_size)
    # 我们将使用掩码变量mask来忽略掉标签为填充项PAD的损失, 初始全1
    mask, num_not_pad_tokens = torch.ones(batch_size,), 0
    l = torch.tensor([0.0])
    for y in Y.permute(1,0): # Y shape: (batch, seq_len)
        dec_output, dec_state = decoder(dec_input, dec_state, enc_outputs)
        l = l + (mask * loss(dec_output, y)).sum()
        dec_input = y  # 使用强制教学
        num_not_pad_tokens += mask.sum().item()
        # EOS后面全是PAD. 下面一行保证一旦遇到EOS接下来的循环中mask就一直是0
        mask = mask * (y != out_vocab.stoi[EOS]).float()
    return l / num_not_pad_tokens

def train(encoder, decoder, dataset, lr, batch_size, num_epochs):
    enc_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)
    dec_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)

    loss = nn.CrossEntropyLoss(reduction='none')
    data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)
    for epoch in range(num_epochs):
        l_sum = 0.0
        for X, Y in data_iter:
            enc_optimizer.zero_grad()
            dec_optimizer.zero_grad()
            l = batch_loss(encoder, decoder, X, Y, loss)
            l.backward()
            enc_optimizer.step()
            dec_optimizer.step()
            l_sum += l.item()
        if (epoch + 1) % 10 == 0:
            print("epoch %d, loss %.3f" % (epoch + 1, l_sum / len(data_iter)))

embed_size, num_hiddens, num_layers = 64, 64, 2
attention_size, drop_prob, lr, batch_size, num_epochs = 10, 0.5, 0.01, 2, 50
encoder = Encoder(len(in_vocab), embed_size, num_hiddens, num_layers,
                  drop_prob)
decoder = Decoder(len(out_vocab), embed_size, num_hiddens, num_layers,
                  attention_size, drop_prob)
train(encoder, decoder, dataset, lr, batch_size, num_epochs)
```

>epoch 10, loss 0.513
>epoch 20, loss 0.243
>epoch 30, loss 0.088
>epoch 40, loss 0.057
>epoch 50, loss 0.035

模型训练完后，进行评估测试。

```python
def translate(encoder, decoder, input_seq, max_seq_len):
    # 将输入序列按空格分割成单词列表
    in_tokens = input_seq.split(' ')
    # 在单词列表后面添加EOS和PAD，使得总长度达到max_seq_len
    in_tokens += [EOS] + [PAD] * (max_seq_len - len(in_tokens) - 1)
    # 将输入单词列表转换为对应的词汇表索引，并创建一个批次大小为1的张量
    enc_input = torch.tensor([[in_vocab.stoi[tk] for tk in in_tokens]]) # batch=1
    # 初始化编码器的状态
    enc_state = encoder.begin_state()
    # 对输入张量进行编码，得到编码器的输出和状态
    enc_output, enc_state = encoder(enc_input, enc_state)
    # 创建解码器的输入，初始值为BOS对应的词汇表索引
    dec_input = torch.tensor([out_vocab.stoi[BOS]])
    # 使用编码器的输出状态初始化解码器的状态
    dec_state = decoder.begin_state(enc_state)
    # 初始化输出单词列表
    output_tokens = []
    # 循环max_seq_len次，每次生成一个输出单词
    for _ in range(max_seq_len):
        # 对解码器输入进行解码，得到解码器的输出和状态
        dec_output, dec_state = decoder(dec_input, dec_state, enc_output)
        # 获取解码器输出中概率最大的单词索引
        pred = dec_output.argmax(dim=1)
        # 将索引转换为对应的单词
        pred_token = out_vocab.itos[int(pred.item())]
        # 如果预测的单词是EOS，则结束循环
        if pred_token == EOS:  # 当任一时间步搜索出EOS时，输出序列即完成
            break
        else:
            # 将预测的单词添加到输出单词列表中
            output_tokens.append(pred_token)
            # 将预测的单词索引作为下一个解码器的输入
            dec_input = pred
    # 返回输出单词列表
    return output_tokens

input_seq = 'ils regardent .'
translate(encoder, decoder, input_seq, max_seq_len)
```

>['they', 'are', 'watching', '.']

评价机器翻译结果通常使用BLEU（Bilingual Evaluation Understudy）。对于模型预测序列中任意的子序列，BLEU考察这个子序列是否出现在标签序列中。

具体来说，设词数为$n$的子序列的精度为$p_n$。它是预测序列与标签序列匹配词数为$n$的子序列的数量与预测序列中词数为$n$的子序列的数量之比。举个例子，假设标签序列为$A$、$B$、$C$、$D$、$E$、$F$，预测序列为$A$、$B$、$B$、$C$、$D$，那么$p_1 = 4/5, p_2 = 3/4, p_3 = 1/3, p_4 = 0$。设$len_{\text{label}}$和$len_{\text{pred}}$分别为标签序列和预测序列的词数，那么，BLEU的定义为

$$ \exp\left(\min\left(0, 1 - \frac{len_{\text{label}}}{len_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},$$

其中$k$是我们希望匹配的子序列的最大词数。可以看到当预测序列和标签序列完全一致时，BLEU为1。

因为匹配较长子序列比匹配较短子序列更难，BLEU对匹配较长子序列的精度赋予了更大权重。例如，当$p_n$固定在0.5时，随着$n$的增大，$0.5^{1/2} \approx 0.7, 0.5^{1/4} \approx 0.84, 0.5^{1/8} \approx 0.92, 0.5^{1/16} \approx 0.96$。另外，模型预测较短序列往往会得到较高$p_n$值。因此，上式中连乘项前面的系数是为了惩罚较短的输出而设的。举个例子，当$k=2$时，假设标签序列为$A$、$B$、$C$、$D$、$E$、$F$，而预测序列为$A$、$B$。虽然$p_1 = p_2 = 1$，但惩罚系数$\exp(1-6/2) \approx 0.14$，因此BLEU也接近0.14。

```python
def bleu(pred_tokens, label_tokens, k):
    # 计算预测序列和标签序列的长度
    len_pred, len_label = len(pred_tokens), len(label_tokens)
    # 计算长度惩罚因子
    score = math.exp(min(0, 1 - len_label / len_pred))
    # 遍历n-gram范围
    for n in range(1, k + 1):
        # 初始化匹配数量和标签子串计数器
        num_matches, label_subs = 0, collections.defaultdict(int)
        # 统计标签序列中n-gram子串出现的次数
        for i in range(len_label - n + 1):
            label_subs[''.join(label_tokens[i: i + n])] += 1
        # 遍历预测序列，检查是否与标签子串匹配
        for i in range(len_pred - n + 1):
            if label_subs[''.join(pred_tokens[i: i + n])] > 0:
                num_matches += 1
                label_subs[''.join(pred_tokens[i: i + n])] -= 1
        # 更新分数
        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
    return score

# 定义一个名为score的函数，接收三个参数：input_seq（输入序列），label_seq（标签序列）和k（用于计算BLEU分数的n-gram值）
def score(input_seq, label_seq, k):
    # 使用translate函数将输入序列翻译成预测的tokens
    pred_tokens = translate(encoder, decoder, input_seq, max_seq_len)
    # 将标签序列按空格分割成tokens
    label_tokens = label_seq.split(' ')
    # 打印BLEU分数和预测的tokens
    print('bleu %.3f, predict: %s' % (bleu(pred_tokens, label_tokens, k),
                                      ' '.join(pred_tokens)))

score('ils regardent .', 'they are watching .', k=2)
score('ils sont canadienne .', 'they are canadian .', k=2)
```

>bleu 0.658, predict: they are arguing .
>
>bleu 0.658, predict: they are actors .

## 总结

可以将编码器—解码器和注意力机制应用于机器翻译中。
BLEU可以用来评价翻译结果。
